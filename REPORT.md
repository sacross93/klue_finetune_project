# KLUE-RE 과제 수행 보고서 (01~07 전체 요약)

## 0. 수행 목표와 전체 구조
본 과제는 KLUE Relation Extraction(관계 추출) 데이터를 기반으로 **EDA → 데이터 구성 → 앱 구조 설계 → LLM 선정 → 구현 → 성능 검토 → 성능 개선 → 과정 정리**까지 전 과정을 수행하고, 각 단계에서 **왜 그 방법을 선택했는지**와 **어떤 결과가 나왔는지**를 논리적으로 설명하는 것을 목표로 한다.

---

## 1. EDA 수행 (01_klue_re_assignment_EDA.ipynb)

### 1-1. 왜 EDA를 먼저 하는가?
- 데이터의 분포/품질/편향을 모르면 모델 선택과 성능 해석이 왜곡된다.
- RE는 라벨 불균형이 심해 **정확도만 보고 성능을 판단하면 착시**가 발생한다.
- 따라서 EDA 단계에서 **라벨 분포, 문장 길이, 엔티티 구간 구조**를 먼저 확인한다.

### 1-2. 핵심 점검 항목
- **라벨 분포**: 특정 라벨(예: no_relation) 쏠림 여부 확인
- **문장 길이 분포**: max_len 결정의 근거 확보
- **엔티티 위치/타입**: 전처리에서 강조해야 할 포인트 확인

### 1-3. 결과 및 해석
- 라벨 분포는 **극도로 불균형**하며, 이는 모델이 정확도만 높이려 할 때 다수 클래스로 치우칠 가능성이 높음을 의미한다.
- 문장 길이 분포를 통해 **MAX_LEN=256**이 정보 손실과 연산 비용 사이의 합리적 선택임을 확인했다.
- EDA 결과로부터 **macro F1을 핵심 지표로 사용해야 한다**는 판단을 내렸다.

---

## 2. 데이터셋 구성 (02_klue_re_dataset_construction.ipynb)

### 2-1. 구성 목적
- KLUE 데이터를 **모델 학습에 적합한 형태**로 변환
- 입력 문장에 엔티티 정보를 명시하여 관계 신호를 강화

### 2-2. 전처리 설계 논리
- RE는 엔티티 위치 정보가 핵심이므로 **[E1]…[/E1], [E2]…[/E2]** 형태로 강조 태그를 삽입
- 라벨을 **정수 인덱스(label_id)**로 매핑하여 학습 일관성 확보
- 전처리 결과를 CSV/JSON으로 저장해 이후 재사용 가능하게 구성

### 2-3. 생성 결과
`temp_result/02_klue_re_dataset_construction/`에 저장:
- `label_mapping.json`: 라벨 ↔ 정수 id 매핑
- `train_sample.csv`: 전처리된 샘플
- `quality_checks.json`: 전처리 품질 점검 결과

### 2-4. 결과 해석
- 입력 텍스트에 엔티티 강조 정보가 포함되어 **관계 예측 신호가 명확해짐**
- 라벨 인덱스 체계가 정리되어 **학습/평가 파이프라인 안정성**이 확보됨

---

## 3. 어플리케이션 구조 설계 (03_klue_re_app_design.ipynb)

### 3-1. 설계 목적
- 모델 성능뿐 아니라 **실서비스 적용 흐름**을 명확히 하기 위함
- 입력 → 전처리 → 추론 → 후처리 → 결과 출력 구조를 정의

### 3-2. 설계 핵심 요소
- **입력**: 문장 + 엔티티 정보
- **전처리**: 엔티티 태그 삽입, 토크나이징
- **모델 추론**: fine‑tuned RE 모델
- **후처리**: label_id → 라벨명 변환
- **출력**: 관계 유형 + 신뢰도

### 3-3. 설계 결과
- RE Task에 적합한 **경량 API 흐름 설계**
- 향후 확장: batch inference, 오류 분석 모듈, confidence threshold 적용 가능

---

## 4. LLM 선정 (Open Source) (04_klue_re_llm_selection.ipynb)

### 4-1. 모델 선택 기준
- 한국어 성능
- 라이선스 적합성
- RE 태스크에서 검증된 아키텍처
- 연산 비용 대비 효율

### 4-2. 왜 인코더(Encoder) 기반 모델을 선택했는가?
- **문제 특성 적합성**: KLUE‑RE는 **고정 라벨(30개) 분류** 문제이며, 인코더는 입력 문장을 **전체적으로 이해한 뒤 분류 헤드로 라벨을 직접 예측**하는 구조라 태스크에 맞다.\n+  반면 디코더 기반 LLM은 생성형 출력 중심이라 **프롬프트 설계/출력 안정성**에 변수가 많고, 정형 분류 과제에서는 오히려 비효율이 크다.
- **재현성/비교 용이성**: 인코더 + 분류 헤드 파이프라인은 학습/평가 절차가 표준화돼 있어 **재현성과 실험 비교**가 쉽다.
- **리소스/개발 속도**: 인코더 모델은 상대적으로 가볍고 학습이 빠르며, **베이스라인→개선 실험**을 반복하기에 현실적이다.

### 4-3. 선정 모델
- `klue/roberta-base`

### 4-4. KLUE‑RoBERTa base 선택 근거
- KLUE 공식 벤치마크에 최적화된 한국어 RoBERTa 계열
- 문장 이해 능력이 높고 downstream fine‑tuning에 적합
- 공개성과 재현성이 높음
 - **small/large 대비 균형**: small은 표현력이 부족할 수 있고, large는 학습 비용이 급증한다. base는 **성능/비용 균형점**이다.
 - **데이터 규모 적합성**: 학습 데이터(약 3만 샘플) 규모에 대해 base가 안정적으로 학습하기에 적절하다.

### 4-5. 고려했으나 제외한 대안: Qwen3 Embedding 8B
- **한국어 임베딩 성능은 우수**하나, 임베딩 모델은 본질적으로 **유사도/검색 최적화** 목적이다.
- RE 과제는 **고정 라벨 분류**이므로, 임베딩 모델을 쓰려면 **벡터 추출 → 별도 분류기(MLP/Linear) 학습** 단계가 추가된다.\n+  이는 파이프라인이 복잡해지고, 실험 반복 비용이 증가한다.
- 8B 모델은 **메모리/추론 비용이 크며**, 과제 환경에서 반복 실험에 비효율적이다.
- 결론적으로 **RE 분류에 직접 최적화된 인코더(PLM) + 분류 헤드** 구조가 더 합리적이라고 판단했다.

---

## 5. 구현

### 5-1. 학습 전 베이스라인 (05_klue_re_no_train_baseline.ipynb)

#### 목적
- 학습 없이 얻는 성능 수준을 확인해 **fine‑tuning 효과의 기준선** 확보

#### 실험
- 다수 클래스(majority)
- 랜덤 균등(random uniform)
- 랜덤 분포 기반(random weighted)

#### 결과 (temp_result/05_klue_re_no_train_baseline/)
- majority: accuracy≈0.596, macro F1≈0.025
- random uniform: accuracy≈0.032
- random weighted: accuracy≈0.195

#### 해석
- 다수 클래스 예측만으로 accuracy가 높게 보이지만, **macro F1은 매우 낮음**
- 이는 **클래스 불균형 착시**를 보여주며, 평가 지표는 macro F1 중심으로 해석해야 함을 확인

---

### 5-2. baseline 학습 (06_klue_re_training_improvement.ipynb)

#### 목적
- 실제 fine‑tuning 수행 및 체크포인트 저장
- 성능 검토 및 개선 실험 기반 확보

#### 결과 저장
- `temp_result/06_klue_re_training/baseline/checkpoint-*`

#### 학습 로그 예시
- Epoch 진행 시 loss 감소 및 성능 상승 확인
- 체크포인트 정상 저장

---

## 6. 성능 검토

### 6-1. 06에서의 평가 도구
- confusion matrix
- per-class report
- error_samples
- metrics_summary

### 6-2. 평가 체크리스트
- 데이터 누수 여부
- 재현성 (seed 고정)
- accuracy/macro F1 균형 확인
- 클래스별 성능 편차 분석
- 오분류 샘플 점검

### 6-3. 07 모델 비교 결과 (07_klue_re_model_comparison.ipynb)
`temp_result/07_model_comparison/metrics_comparison.csv` 기준:

| 모델 | Accuracy | Macro F1 | 의미 |
|------|----------|----------|------|
| Base (학습 전) | ≈0.015 | ≈0.001 | 분류 head 랜덤 초기화 → 거의 랜덤 수준 |
| Fine‑tuned | ≈0.749 | ≈0.599 | 관계 패턴 학습 성공 |

#### 해석
- 학습 전 모델은 **분류 head가 랜덤 초기화** 상태이므로 성능이 거의 무작위 수준
- fine‑tuning 이후에는 관계 패턴이 학습되어 성능이 크게 개선됨
- macro F1이 accuracy보다 낮다는 것은 **소수 클래스에서 오분류가 많음**을 의미
- **05번 베이스라인(accuracy≈0.596)과 07번 Base(accuracy≈0.015)의 차이**는 기준이 다르기 때문임:    
05번은 **다수 클래스만 찍는 규칙 기반 베이스라인**이라 클래스 불균형 덕분에 정확도가 높게 나온 것이고,    
07번은 **학습 전 모델(랜덤 분류 head)**의 실제 예측 성능이라 거의 랜덤 수준으로 낮게 나온 것이다.    
    
---    

## 7. 성능 개선

### 7-1. 개선 필요성
- macro F1이 낮은 이유는 **소수 클래스 성능 부족**
- accuracy만 높이는 방식은 실무적으로 부적절

### 7-2. 적용 개선안
- **class‑weighted loss (WeightedTrainer)** 적용
- 소수 클래스 손실 비중을 높여 모델이 다수 클래스만 학습하는 현상을 완화

### 7-3. 추가 개선 방향
- 데이터 재샘플링 (over/under‑sampling)
- Focal Loss
- label smoothing
- 데이터 증강
- 라벨별 오류 분석 기반 수정

---

## 8. 과정 정리 (결론)

1. EDA를 통해 데이터 불균형을 확인했고, macro F1 중심 평가가 필수임을 확인했다.
2. 데이터 전처리에서 엔티티 정보 강조를 통해 관계 예측 신호를 강화했다.
3. 한국어 RE에 적합한 `klue/roberta-base`를 선정했다.
4. 학습 전 베이스라인은 accuracy 착시를 보여 주었고, 모델 학습 필요성을 확인했다.
5. fine‑tuning 결과는 학습 전 모델 대비 성능이 압도적으로 향상됨을 보여 주었다.
6. macro F1 < accuracy는 클래스 불균형 문제를 시사하며, 소수 클래스 개선이 주요 과제임을 의미한다.
7. 따라서 **사전학습 모델 단독 사용은 불충분하며, fine‑tuning이 필수**이고, 이후 개선은 소수 클래스 중심으로 접근해야 한다.

---

## (선택) 보고서 품질 강화 제안
- confusion matrix를 활용해 **특정 라벨 간 혼동 패턴**을 설명
- error_samples에서 대표 오분류 사례를 선택해 **정성 분석** 추가
- 개선 실험(가중치 적용 등) 결과를 표로 정리해 **전후 비교** 강조
